# -*- coding: utf-8 -*-
"""eda_pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DrR8BTAP3hBXwxjiL6fJdIv0-0yuCwtd
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from collections import Counter
from wordcloud import WordCloud
from sklearn.feature_extraction.text import CountVectorizer
import re

# -------- 1. Load data --------
def load_data(path):
    df = pd.read_excel(path)
    queries = df['Queries'].dropna().str.strip().str.lower()
    return queries


queries = load_data("SkyRocket Data.xlsx")

# -------- 2. Basic Profile --------
def profile_dataset(queries):
    total = len(queries)
    nulls = queries.isnull().sum()
    duplicates = queries.duplicated().sum()
    unique = queries.nunique()

    print(f"Total queries: {total}")
    print(f"Null entries: {nulls}")
    print(f"Exact duplicates: {duplicates}")
    print(f"Unique queries: {unique}")

    if duplicates > 0:
        print("\nüîÅ Exact Duplicates Found:")
        dup_rows = queries[queries.duplicated(keep=False)]  # all duplicate rows, not just second instances
        dup_df = pd.DataFrame({
            'Row Index': dup_rows.index + 2,
            'Query': dup_rows.values
        }).sort_values(by='Query')
        print(dup_df.to_string(index=False))




profile_dataset(queries)

# -------- 3. Query Length Stats --------



def analyze_query_lengths(queries):
    # Lengths of each query
    lengths = queries.str.len()

    # Compute frequency of each length
    length_counts = Counter(lengths)
    lengths_sorted = sorted(length_counts.items())  # List of (length, count) tuples

    x = [item[0] for item in lengths_sorted]
    y = [item[1] for item in lengths_sorted]

    # Print basic stats
    print(f"Min length: {min(x)}")
    print(f"Max length: {max(x)}")
    print(f"Average length: {sum([a * b for a, b in zip(x, y)]) / sum(y):.2f}")

    # Plot
    plt.figure(figsize=(14, 6))
    bars = plt.bar(x, y, color='mediumslateblue', edgecolor='black')

    # Add frequency labels on top of each bar
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2, height + 1, f'{int(height)}',
                 ha='center', va='bottom', fontsize=8, rotation=0)

    # X-ticks: every query length
    plt.xticks(x, rotation=90)
    plt.xlabel("Query Length (characters)")
    plt.ylabel("Frequency (exact count)")
    plt.title("Exact Distribution of Query Lengths")
    plt.tight_layout()
    plt.show()




analyze_query_lengths(queries)

# -------- 4. word Length Stats --------

def analyze_word_counts(queries):
    # Count words per query
    word_counts = queries.apply(lambda x: len(x.split()))
    word_count_freq = Counter(word_counts)
    counts_sorted = sorted(word_count_freq.items())  # (word_count, frequency)

    x = [item[0] for item in counts_sorted]
    y = [item[1] for item in counts_sorted]

    # Print summary stats
    print(f"Min words per query: {min(x)}")
    print(f"Max words per query: {max(x)}")
    print(f"Average words per query: {sum([a * b for a, b in zip(x, y)]) / sum(y):.2f}")

    # Plot
    plt.figure(figsize=(14, 6))
    bars = plt.bar(x, y, color='darkorange', edgecolor='black')

    # Add frequency labels on each bar
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2, height + 1, f'{int(height)}',
                 ha='center', va='bottom', fontsize=8)

    plt.xticks(x)
    plt.xlabel("Number of Words per Query")
    plt.ylabel("Frequency (Exact Count)")
    plt.title("Distribution of Word Counts in Queries")
    plt.tight_layout()
    plt.show()

analyze_word_counts(queries)

# -------- 4. N-gram Frequency (Unigram/Bigrams) --------
def extract_word_stats(queries, n=1, top_k=20):
    vec = CountVectorizer(ngram_range=(n, n), stop_words='english')
    X = vec.fit_transform(queries)
    freqs = zip(vec.get_feature_names_out(), X.sum(axis=0).tolist()[0])
    sorted_freqs = sorted(freqs, key=lambda x: x[1], reverse=True)

    print(f"\nTop {top_k} {n}-grams:")
    for term, freq in sorted_freqs[:top_k]:
        print(f"{term}: {freq}")



extract_word_stats(queries, n=1)

# -------- 5. Word Cloud --------
def show_wordcloud(queries):
    all_text = ' '.join(queries.tolist())
    cloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)
    plt.figure(figsize=(12, 6))
    plt.imshow(cloud, interpolation='bilinear')
    plt.axis("off")
    plt.title("Word Cloud of All Queries")
    plt.show()


show_wordcloud(queries)

# -------- 6. Word cloud for misspellings --------

import pandas as pd
import nltk
import re
from nltk.corpus import words as nltk_words, wordnet
from nltk.stem import WordNetLemmatizer

# Download necessary NLTK data (run once)
nltk.download('words')
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('averaged_perceptron_tagger')

# Load English vocabulary set:
english_vocab = set(w.lower() for w in nltk_words.words())

# Initialize lemmatizer
lemmatizer = WordNetLemmatizer()

# Map NLTK POS tags to WordNet POS tags:
def get_wordnet_pos(word):
    """Return WordNet POS tag from NLTK POS notation for better lemmatization."""
    tag = nltk.pos_tag([word])[0][1][0].upper()
    tag_dict = {
        'J': wordnet.ADJ,
        'N': wordnet.NOUN,
        'V': wordnet.VERB,
        'R': wordnet.ADV
    }
    return tag_dict.get(tag, wordnet.NOUN)

def tokenize(text):
    """Simple tokenizer to extract words in lowercase."""
    return re.findall(r'\b[a-z]+\b', text.lower())

def detect_misspellings(df, query_col='Queries', exclude_top_n=0):
    misspelling_counts = {}

    for query in df[query_col].dropna().astype(str):
        tokens = tokenize(query)
        for word in tokens:
            if len(word) <= 2:
                continue  # skip very short words like "I", "to", "at"
            if word in english_vocab:
                continue  # correct spelling
            # Lemmatize word with POS tag
            lemma = lemmatizer.lemmatize(word, get_wordnet_pos(word))
            if lemma in english_vocab:
                continue  # lemma recognized, so word considered correct
            # Mark as misspelled
            misspelling_counts[word] = misspelling_counts.get(word, 0) + 1

    # Sort misspellings by frequency descending:
    sorted_misspellings = sorted(misspelling_counts.items(), key=lambda x: x[1], reverse=True)

    # Print top 10 most frequent misspellings overall:
    print(f"Top 10 misspellings overall:")
    for word, count in sorted_misspellings[:10]:
        print(f"{word}: {count} occurrences")

    # Exclude top n frequent misspellings if desired:
    if exclude_top_n > 0:
        filtered_misspellings = sorted_misspellings[exclude_top_n:]
    else:
        filtered_misspellings = sorted_misspellings

    print(f"\nMisspellings list after excluding top {exclude_top_n} most frequent:")
    for word, count in filtered_misspellings:
        print(f"{word}: {count}")

    # Return lists for programmatic use if needed
    return sorted_misspellings, filtered_misspellings

# Usage example:

# Load your Excel file - adjust path as needed
df = pd.read_excel('SkyRocket Data.xlsx')

# Call the function, e.g., exclude top 5 frequent misspellings:
overall_misspellings, filtered_misspellings = detect_misspellings(df, exclude_top_n=5)

# output the final filtered_misspellings list in a dataframe to a csv file
filtered_df = pd.DataFrame(filtered_misspellings, columns=['Misspelling', 'Count'])
filtered_df.to_csv('filtered_misspellings.csv', index=False)

import matplotlib.pyplot as plt
from wordcloud import WordCloud

# Prepare a dictionary of misspellings and their frequencies
word_freq = dict(filtered_misspellings)  # from your existing misspellings list

# Generate the word cloud
wordcloud = WordCloud(width=800, height=400,
                      background_color='white',
                      colormap='viridis').generate_from_frequencies(word_freq)

# Display the word cloud
plt.figure(figsize=(15, 7))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud of Misspelled Words in Customer Queries')
plt.show()

# -------- 7. Template Phrase Extractor --------
def find_common_templates(queries, top_k=15):
    templates = []
    patterns = [
        r'how do i \w+',
        r'i want to \w+',
        r'can you \w+',
        r'please \w+',
        r'i need \w+',
        r'what is \w+'
    ]
    for query in queries:
        for p in patterns:
            match = re.search(p, query)
            if match:
                templates.append(match.group())

    template_series = pd.Series(templates)
    top_templates = template_series.value_counts().head(top_k)
    print("\nTop repeated templates:")
    print(top_templates)


find_common_templates(queries)















